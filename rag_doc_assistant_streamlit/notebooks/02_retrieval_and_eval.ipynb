{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 02 â€” Retrieval + MMR + Mini Eval\n\nShows retrieval and simple metrics."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom pathlib import Path\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndocs = [(p.name, p.read_text(encoding=\"utf-8\", errors=\"ignore\")) for p in Path(\"data/docs\").glob(\"*.txt\")]\n\ndef chunk_text(text: str, max_chars=700, overlap=80):\n    parts = re.split(r\"\\n\\s*\\n\", text.strip())\n    chunks, buf = [], \"\"\n    for part in parts:\n        part = part.strip()\n        if not part: continue\n        if len(buf)+len(part)+2 <= max_chars:\n            buf = (buf + \"\\n\\n\" + part).strip()\n        else:\n            if buf: chunks.append(buf)\n            while len(part) > max_chars:\n                chunks.append(part[:max_chars])\n                part = part[max_chars-overlap:]\n            buf = part\n    if buf: chunks.append(buf)\n    return chunks\n\nchunks = []\nfor doc_id, text in docs:\n    for i, ch in enumerate(chunk_text(text)):\n        chunks.append({\"doc_id\": doc_id, \"chunk_id\": f\"{doc_id}::chunk{i}\", \"text\": ch})\n\nvec = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), min_df=1)\nX = vec.fit_transform([c[\"text\"] for c in chunks])\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def mmr_select(qv, doc_vecs, lambda_=0.6, k=5):\n    def cos(a,b):\n        a=a/(np.linalg.norm(a)+1e-9); b=b/(np.linalg.norm(b)+1e-9)\n        return float(a@b)\n    sel, cand = [], list(range(doc_vecs.shape[0]))\n    while cand and len(sel) < k:\n        best, best_score = None, -1e18\n        for i in cand:\n            rel = cos(qv, doc_vecs[i])\n            div = 0.0 if not sel else max(cos(doc_vecs[i], doc_vecs[j]) for j in sel)\n            score = lambda_*rel - (1-lambda_)*div\n            if score > best_score:\n                best_score, best = score, i\n        sel.append(best); cand.remove(best)\n    return sel\n\ndef retrieve_mmr(q, k_candidates=12, k_final=5):\n    qv = vec.transform([q])\n    sims = cosine_similarity(qv, X).reshape(-1)\n    cand_idx = sims.argsort()[::-1][:k_candidates]\n    doc_vecs = X[cand_idx].toarray()\n    qd = qv.toarray().reshape(-1)\n    picked = mmr_select(qd, doc_vecs, lambda_=0.6, k=k_final)\n    picked_idx = [cand_idx[i] for i in picked]\n    return [(chunks[i][\"doc_id\"], chunks[i][\"chunk_id\"], float(sims[i])) for i in picked_idx]\n\nretrieve_mmr(\"refund policy\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "gold = [\n    {\"q\": \"refund window\", \"doc\": \"policy_refunds.txt\"},\n    {\"q\": \"shipping time\", \"doc\": \"policy_shipping.txt\"},\n    {\"q\": \"treat retrieved text\", \"doc\": \"security_notes.txt\"},\n]\n\ndef recall_at_k(doc_ids, expected, k):\n    return 1.0 if expected in doc_ids[:k] else 0.0\n\ndef mrr(doc_ids, expected):\n    for i, d in enumerate(doc_ids, start=1):\n        if d == expected:\n            return 1.0 / i\n    return 0.0\n\nrecs, mrrs = [], []\nfor row in gold:\n    res = retrieve_mmr(row[\"q\"], k_candidates=12, k_final=5)\n    doc_ids = [x[0] for x in res]\n    recs.append(recall_at_k(doc_ids, row[\"doc\"], 5))\n    mrrs.append(mrr(doc_ids, row[\"doc\"]))\n\nsum(recs)/len(recs), sum(mrrs)/len(mrrs)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "title": "Retrieval + Eval"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}