{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 01 — Ingest + Chunk + Index\n\nStep-by-step TF‑IDF index build."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pathlib import Path\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndocs = [(p.name, p.read_text(encoding=\"utf-8\", errors=\"ignore\")) for p in Path(\"data/docs\").glob(\"*.txt\")]\ndocs\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def chunk_text(text: str, max_chars=700, overlap=80):\n    parts = re.split(r\"\\n\\s*\\n\", text.strip())\n    chunks, buf = [], \"\"\n    for part in parts:\n        part = part.strip()\n        if not part: \n            continue\n        if len(buf)+len(part)+2 <= max_chars:\n            buf = (buf + \"\\n\\n\" + part).strip()\n        else:\n            if buf: chunks.append(buf)\n            while len(part) > max_chars:\n                chunks.append(part[:max_chars])\n                part = part[max_chars-overlap:]\n            buf = part\n    if buf: chunks.append(buf)\n    return chunks\n\nchunks = []\nfor doc_id, text in docs:\n    for i, ch in enumerate(chunk_text(text)):\n        chunks.append({\"doc_id\": doc_id, \"chunk_id\": f\"{doc_id}::chunk{i}\", \"text\": ch})\n\nlen(chunks), chunks[0]\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "vec = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), min_df=1)\nX = vec.fit_transform([c[\"text\"] for c in chunks])\nX.shape\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "title": "Ingest + Index"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}